---
title: "Testing the Reasoning Model"
---

<Note>
  This model identifies directional trends by predicting how likely price action in the selected timeframe will propagate to lower-frequency timeframes, distinguishing structurally significant market moves from noise.
</Note>

<Tip>
  We recommend using higher frequency data compared to your target forecast horizon, as this leverages the model's propagation methodology effectively (e.g., use 15 or 30 min data to predict daily moves).
</Tip>

## Select Time Series Data

<AccordionGroup>
  <Accordion title="Upload Data">
    ```python
    import pandas as pd
    
    df = pd.read_csv("filename.csv")[['datetime','open','high','low','close']]
    
    print(df)
    
    ```
  </Accordion>
  <Accordion title="Download Sample Data">
    ```python
    import requests
    import pandas as pd
    import io
    
    def download_github_data(url):
        response = requests.get(url)
        if response.status_code == 200:
            return pd.read_csv(io.StringIO(response.text))
        else:
            raise Exception(f"Failed to download data: {response.status_code}")
    
    # Example usage
    github_url = "https://raw.githubusercontent.com/username/repo/main/market_data.csv"
    market_data = download_github_data(github_url)
    print(f"Downloaded {len(market_data)} rows of data")
    ```
  </Accordion>
</AccordionGroup>

## Data Preparation

<AccordionGroup>
  <Accordion title="Create JSON Payload - 1 API Call">
    ```python
    import pandas as pd
    import json
    
    def time_series_dict(df,interval,interval_unit,reasoning_mode):
    
      return {
          "datetime": df['datetime'].tolist(),
          "open": df['open'].tolist(),
          "high": df['high'].tolist(),
          "low": df['low'].tolist(),
          "close": df['close'].tolist(),
          "interval": interval,
          "interval_unit":interval_unit,
          "reasoning_mode":reasoning_mode
      }
    
    df = pd.read_csv("filename.csv")[['datetime','open','high','low','close']]
    
    interval = 1 
    
    interval_unit = "minutes" # options: seconds,minutes,days
    
    reasoning_mode = "reactive" # options: proactive or reactive
    
    json_payload = time_series_dict(df,interval,interval_unit,reasoning_mode)
    
    ```
  </Accordion>
  <Accordion title="Create JSON Payload - Multiple API Calls">
    ```python
    import pandas as pd
    import numpy as np
    import json
    
    def time_series_dict(df, interval, interval_unit, reasoning_mode):
       """
       Converts financial time series data to a dictionary format suitable for API submission.
       
       Parameters:
       - df: DataFrame containing OHLC price data with datetime column
       - interval: Integer representing the time interval size
       - interval_unit: String specifying the unit of time (seconds, minutes, days)
       - reasoning_mode: String indicating model behavior (proactive or reactive)
       
       Returns:
       - Dictionary with time series data and configuration parameters
       """
       return {
           "datetime": df['datetime'].tolist(),
           "open": df['open'].tolist(),
           "high": df['high'].tolist(),
           "low": df['low'].tolist(),
           "close": df['close'].tolist(),
           "interval": interval,
           "interval_unit": interval_unit,
           "reasoning_mode": reasoning_mode
       }
    
    # Load the financial dataset with only the required columns for efficiency
    df = pd.read_csv("filename.csv")[['datetime', 'open', 'high', 'low', 'close']]
    
    # Configure time series parameters
    interval = 1 
    interval_unit = "minutes"  # options: seconds, minutes, days
    reasoning_mode = "reactive"  # options: proactive or reactive
    
    # Split data for efficient submission
    chunk_size = 5000
    df_list = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    
    # Create API-ready payloads for each data chunk
    json_payloads = [time_series_dict(chunk, interval, interval_unit, reasoning_mode) 
                   for chunk in df_list]
    
    ```
  </Accordion>
</AccordionGroup>

## Leverage API endpoint

<AccordionGroup>
  <Accordion title="Submit Data to API - 1 API Call">
    ```python
    import pandas as pd
    import json
    import os
    from datetime import datetime
    
    def log_forecast_check(task_id, forecast_date, log_file="sumtyme_task_ids.log"):
        """
        Logs a forecast check to a CSV file.
        
        Parameters:
        - task_id (str): The ID of the task 
        - forecast_date (str): The date of the forecast
        - log_file (str): Path to the log file, defaults to "sumtyme_task_ids.log"
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Create log file with header if it doesn't exist
        if not os.path.exists(log_file):
            with open(log_file, 'w') as f:
                f.write("timestamp,forecast_date,task_id,checked\n")
    
        # Write to log file
        with open(log_file, 'a') as f:
            f.write(f"{current_time},{forecast_date},{task_id},blank\n")
        
        return True
    
    def time_series_dict(df,interval,interval_unit,reasoning_mode):
    
      return {
          "datetime": df['datetime'].tolist(),
          "open": df['open'].tolist(),
          "high": df['high'].tolist(),
          "low": df['low'].tolist(),
          "close": df['close'].tolist(),
          "interval": interval,
          "interval_unit":interval_unit,
          "reasoning_mode":reasoning_mode
      }
    
    def post_request(df,interval,interval_unit,reasoning_mode):
    
      # Prepare request
      api_endpoint = f"https://www.sumtyme.com/shared/v1/reasoning-model-ts"
      headers = {"Content-Type": "application/json"}
      
      # Create payload from data
      payload = time_series_dict(df,interval,interval_unit,reasoning_mode)
      
      # Send POST request to API
      response = requests.post(api_endpoint, json=payload, headers=headers)
    
      if response.status_code != 200:
                print(f"API request failed for {forecast_date} with status code {response.status_code}: {response.text}")
      else:
        return response.json()
    
    df = pd.read_csv("filename.csv")[['datetime','open','high','low','close']]
    
    forecast_date = df['datetime'].iloc[-1]
    
    interval = 1 
    
    interval_unit = "minutes" # options: seconds,minutes,days
    
    reasoning_mode = "reactive" # options: proactive or reactive
    
    response = post_request(df,interval,interval_unit,reasoning_mode)
    
    if response:
    
      task_id = response.get("task_id")
    
      log_forecast_check(task_id,forecast_date)
        
    ```
  </Accordion>
</AccordionGroup>

## Initial API Response

<AccordionGroup>
  <Accordion title="Get Task ID">
    ```python
    
        
    ```
  </Accordion>
</AccordionGroup>

## Retrieve Analysis

<AccordionGroup>
  <Accordion title="Using Log File">
    ```python
    
    
    import pandas as pd
    import requests
    
    def check_task_status(task_id):
      """Check the status of a submitted task"""
      api_endpoint = f"https://www.sumtyme.com/results/{task_id}"
      
      # Make the API request
      response = requests.get(api_endpoint)
      
      # Check if the request was successful (status code 200)
      if response.status_code == 200:
          result_data = response.json()
          
          # Check if processing is complete
          if "status" in result_data and result_data["status"] == "complete":
              return result_data
      
      # If we get here, either the request failed or status wasn't complete
      print(f"Failed to retrieve task status for {task_id}: {response.status_code}")
      return None
    
    def dict_to_dataframe(response_dict):
      """Transform response dictionary to dataframe more efficiently"""
      # Extract the result dictionary
      result_dict = response_dict['result']
      
      # Create the DataFrame directly from the dictionary
      df = pd.DataFrame([
          {'datetime': timestamp, 'trend_identified': data['trend_identified']}
          for timestamp, data in result_dict.items()
      ])
      
      # Convert datetime strings to pandas datetime objects
      df['datetime'] = pd.to_datetime(df['datetime'])
      
      # Sort by datetime
      return df.sort_values('datetime').reset_index(drop=True)
    
    def get_analysis(dataset, save_filename):
      """Process the dataset and check task statuses"""
      # Create a copy of the dataset to update
      updated_dataset = dataset.copy()
      
      # Pre-allocate the 'checked' column with default value
      updated_dataset['checked'] = 'Not processed'
      
      # Process each row in the dataset
      for i in dataset.index:
          try:
              # Extract data for current row
              task_id = str(dataset.loc[i, 'task_id'])
              forecast_date = pd.to_datetime(dataset.loc[i, 'forecast_date'])
              
              # Get analysis results
              analysis_results = check_task_status(task_id)
              
              # Process results if available
              if analysis_results is not None and 'result' in analysis_results:
                  # Convert results to DataFrame
                  df_results = dict_to_dataframe(analysis_results)
                  
                  # Check if dataframe has data
                  if not df_results.empty:
                      # Get the last datetime and trend in the results
                      last_datetime = df_results['datetime'].iloc[-1]
                      trend = df_results['trend_identified'].iloc[-1]
                      
                      # Update the 'checked' column based on datetime match
                      if last_datetime == forecast_date:
                          updated_dataset.loc[i, 'checked'] = trend
                      else:
                          updated_dataset.loc[i, 'checked'] = 'No'
                  else:
                      updated_dataset.loc[i, 'checked'] = 'Empty results'
              else:
                  updated_dataset.loc[i, 'checked'] = 'No results'
                  
          except Exception as e:
              # Improved error handling with exception details
              print(f"Error processing task_id {task_id}: {str(e)}")
              updated_dataset.loc[i, 'checked'] = 'Error'
      
      # Save the updated dataset
      updated_dataset.to_csv(save_filename, index=False)
      print(f"Processing complete. Results saved to {save_filename}")
    
    filename = "spy_1_min_task_ids"
    input_file = f"{filename}.csv"
    save_filename = f"{filename}_checked.csv"
        
    # Load the dataset
    dataset = pd.read_csv(input_file)
    
    get_analysis(dataset, save_filename)
      
        
    ```
  </Accordion>
  <Accordion title="Using Task ID">
    ```python
    
    
    import pandas as pd
    import requests
    
    def check_task_status(task_id):
      """Check the status of a submitted task"""
      api_endpoint = f"https://www.sumtyme.com/results/{task_id}"
      
      # Make the API request
      response = requests.get(api_endpoint)
      
      # Check if the request was successful (status code 200)
      if response.status_code == 200:
          result_data = response.json()
          
          # Check if processing is complete
          if "status" in result_data and result_data["status"] == "complete":
              return result_data
    
          elif "status" in result_data and result_data["status"] == "processing":
               print(f"{task_id}: Still processing") 
               return None
      
      # If we get here, either the request failed or status wasn't complete
      print(f"Failed to retrieve task status for {task_id}: {response.status_code}")
      return None
    
    def dict_to_dataframe(response_dict):
      """Transform response dictionary to dataframe more efficiently"""
      # Extract the result dictionary
      result_dict = response_dict['result']
      
      # Create the DataFrame directly from the dictionary
      df = pd.DataFrame([
          {'datetime': timestamp, 'trend_identified': data['trend_identified']}
          for timestamp, data in result_dict.items()
      ])
      
      # Convert datetime strings to pandas datetime objects
      df['datetime'] = pd.to_datetime(df['datetime'])
      
      # Sort by datetime
      return df.sort_values('datetime').reset_index(drop=True)
    
              
    
    task_id = ""
    
    result = check_task_status(task_id)
    
    if result is not None:
    
      final = dict_to_dataframe(response_dict)
    
      final.to_csv("sumtyme_analysis.csv",index=False)
    
        
    ```
  </Accordion>
</AccordionGroup>

## Plot Forecasts

<AccordionGroup>
  <Accordion title="Merge Price Data and Log File/Saved DataFrame">
    ```python
    
        
    ```

    ```python
    
        
    ```
  </Accordion>
  <Accordion title="Plot Data">
    ```python
    
        
    ```
  </Accordion>
</AccordionGroup>

## Full Script

<AccordionGroup>
  <Accordion title="Single API Call">
    ```python
    
        
    ```
  </Accordion>
  <Accordion title="Multiple API Calls">
    ```python
    
        
    ```
  </Accordion>
</AccordionGroup>

## Performance Evaluation

<AccordionGroup>
  <Accordion title="Predictive Lead Time">
    Quantify how many minutes/hours/days before major market events the model provides reliable signals, demonstrating its ability to anticipate systemic shifts before they manifest in price action.
  </Accordion>
  <Accordion title="Reaction Speed">
    Measure how quickly the model adjusts after sudden market shocks, calculated as the time between a shock event and when the model's predictions align with the new reality.
  </Accordion>
  <Accordion title="Time Horizon Alignment">
    Verify that predictions for neighbouring time periods (e.g. 1 hour, 2 hour, 3 hour) consistently build upon each other without logical contradictions or conflicting signals.
  </Accordion>
</AccordionGroup>

## Tips

<AccordionGroup>
  <Accordion title="Interpreting Trend Signals">
    Each forecast should be interpreted within its trend context: the first signal in a consecutive trend marks the start of a trend, while consecutive forecasts in the same direction indicate the consistency of the current trend.
  </Accordion>
  <Accordion title="Understanding Forecast Context">
    The reasoning model begins its analysis with as little as 10 historical data points. Your results include both the forecast (if identified) and the trend context that shaped it, all in one view. This helps you understand if predictions occur at the beginning, middle, or end of broader trends, making forecasts easier to understand and increasing your confidence in decisions.
  </Accordion>
  <Accordion title="Cross-Timeframe Connectivity">
    The reasoning model operates as a unified system where predictions across multiple time horizons work together rather than independently. This integrated approach reveals how market movements propagate across timeframes, uncovering interconnected relationships that traditional single-timeframe approaches fundamentally cannot detect.

    <img
      height="200"
      src="/images/Unified System Architecture.png"
    />
  </Accordion>
  <Accordion title="Immediate Insights, Custom Optimisation">
    The reasoning model delivers immediate market insights without requiring market-specific training data. While predictive out of the box, it can be fine-tuned to optimise performance for specific time horizons through customised post-training. This flexibility allows users to incorporate multiple timeframes or any combination of traditional and alternative datasets of their choosing.
  </Accordion>
  <Accordion title="Sandbox Queue System">
    The sandbox operates on a FIFO (first-in, first-out) queue system, where each new API request joins the queue and waits for processing. During high-usage periods, wait times may increase. If you need to test in a dedicated environment without queue delays, contact us at [team@sumtyme.ai](mailto:team@sumtyme.ai).
  </Accordion>
  <Accordion title="High-to-Low Frequency Propagation Assumption">
    The reasoning model assumes that all price movements originate in high-frequency time series before cascading into lower-frequency time series. It therefore analyses price action and only returns trends that it believes will translate into lower frequency timeframes.
  </Accordion>
</AccordionGroup>